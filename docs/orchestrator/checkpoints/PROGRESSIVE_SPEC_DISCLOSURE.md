# Progressive Spec Disclosure System

**Philosophy**: Adaptive escalation mimicking senior/junior developer relationship. Start minimal, provide increasingly detailed guidance only as needed.

---

## Phase 1: Initial Spec (Minimal)

**Generated by**: gpt-oss:20b (local, $0)
**Timing**: Once at experiment start
**Cost**: Free

**Prompt structure**:
```
Generate a MINIMAL implementation specification for the following coding problem:

{problem}

Your spec should include ONLY:
1. Problem analysis - What is the core challenge? What are the constraints?
2. Implementation specification - What should the function do? What should it return?

CRITICAL - Do NOT include:
- Implementation approach or algorithm suggestions
- Edge cases or common pitfalls
- Test criteria or examples
- Reference implementation, pseudocode, or code skeleton
- Specific data structures or algorithms to use
```

**Example output** (HumanEval/0):
```
Problem analysis
Determine whether any two distinct numbers in a given list are closer to each
other than a specified distance. The task is to decide if there exists at least
one pair of elements whose absolute difference is strictly less than the threshold.

Implementation specification
`has_close_elements(numbers: List[float], threshold: float) -> bool`
- Returns `True` if there exists at least one pair of distinct indices `i ≠ j`
  such that `abs(numbers[i] - numbers[j]) < threshold`.
- Returns `False` otherwise.
```

**Length**: ~700 chars (vs. 7,300 with old detailed spec)
**Generation time**: ~18s (vs. 41s with old spec)

---

## Checkpoint 1 (Iteration 5): Strategic Guidance

**Generated by**: Claude Sonnet 4.5 (frontier, ~$3-15/1M tokens)
**Timing**: At checkpoint_interval (default: every 5 iterations)
**Cost**: ~$0.05-0.10 per checkpoint

**Combines**:
1. **Current debugging analysis** (4 sections):
   - Failing test analysis (what tests fail, pass rate)
   - Root cause diagnosis (step-by-step code trace)
   - Proposed fix (specific code changes)
   - Edge cases to verify (after fix)

2. **+ Progressive spec section** (NEW - adaptive to failures):

```
**STRATEGIC GUIDANCE FROM SENIOR DEVELOPER:**

## Implementation Approach
What high-level algorithm or strategy should be used?
(e.g., "sorting enables O(n log n)", "use hash map for O(1) lookup")
Consider what the failed attempts reveal about the model's understanding.

## Edge Cases to Handle
What special scenarios must be handled?
(empty input, single element, duplicates, boundary values, etc.)
Focus on edge cases that seem to be causing failures based on error patterns.

## Common Pitfalls
What mistakes should be avoided?
(off-by-one errors, wrong operators, performance issues, etc.)
Highlight pitfalls evident in the failed attempts.
```

**Key features**:
- **Adaptive**: Generated based on *actual* failure patterns, not generic
- **Strategic**: Suggests approach, not implementation details
- **Targeted**: Focuses on edge cases causing current failures

---

## Checkpoint 2 (Iteration 10): Test Criteria

**Generated by**: Claude Sonnet 4.5 (frontier)
**Timing**: At 2nd checkpoint
**Cost**: ~$0.05-0.10 per checkpoint

**Combines**:
1. **Current debugging analysis** (same 4 sections as Checkpoint 1)

2. **+ Progressive spec section** (NEW - adaptive to failures):

```
**TEST CRITERIA FROM SENIOR DEVELOPER:**

## Test Criteria & Expected Behavior
What are specific test cases that should pass? Include:
- Normal cases with expected outputs
- Edge cases with expected behavior
- Corner cases the model might miss

Focus on tests that would catch the types of errors seen in the failed attempts.
```

**Key features**:
- **Adaptive**: Test cases chosen based on failure patterns
- **Tactical**: Shows specific inputs/outputs the model is missing
- **Focused**: Representative tests, not exhaustive coverage

---

## Checkpoint 3+ (Iteration 15+): Debugging Only

**Generated by**: Claude Sonnet 4.5 (frontier)
**Timing**: Every subsequent checkpoint
**Cost**: ~$0.05-0.10 per checkpoint

**Contains**: Only the 4-section debugging analysis (no new spec sections)

**Rationale**: By checkpoint 3, the model has received:
- Initial spec (what to do)
- Strategic guidance (how to approach it)
- Test criteria (what correctness looks like)

Further checkpoints provide only tactical debugging help, not new strategic info.

---

## Cost Analysis

### Scenario: Problem solved in 15 iterations

**Breakdown**:
- Initial spec: gpt-oss:20b (local, $0)
- Iterations 1-4: qwen2.5-coder:32b (local, $0)
- Checkpoint 1 (iter 5): Sonnet 4.5 debugging + strategic guidance (~$0.08)
- Iterations 6-9: qwen2.5-coder:32b (local, $0)
- Checkpoint 2 (iter 10): Sonnet 4.5 debugging + test criteria (~$0.08)
- Iterations 11-14: qwen2.5-coder:32b (local, $0)
- Checkpoint 3 (iter 15): Sonnet 4.5 debugging only (~$0.05)

**Total cost**: ~$0.21

**Comparison**:
- **Iterative (this system)**: $0.21
- **Direct Sonnet solve**: $0.01 (21x cheaper)

### When This System Makes Sense

**Economic justification requires**:
1. **High local success rate** (>70% solve without checkpoints, $0 cost path)
2. **Capability building** (investment in improving local models over time)
3. **Research value** (testing adaptive escalation architectures)

**Not economically justified if**:
- Most problems require 2+ checkpoints
- Goal is just to solve problems (use frontier directly)
- No learning/improvement feedback loop

---

## Implementation Details

**Files modified**:
1. `koderz/models/frontier.py:67-255` - Added progressive spec generation
2. `koderz/models/openai_client.py:65-193` - Added progressive spec generation
3. `koderz/orchestrator.py:699-809` - Pass checkpoint_num and problem_prompt

**Key parameters**:
- `checkpoint_num`: Which checkpoint (1, 2, 3, etc.)
- `problem_prompt`: Original problem for adaptive spec generation

**Adaptive generation**:
- Checkpoint 1: Analyzes failed attempts → generates relevant strategic guidance
- Checkpoint 2: Analyzes failed attempts → generates relevant test criteria
- Checkpoint 3+: Only debugging analysis (no new spec)

---

## Design Philosophy

**Senior/Junior Developer Relationship**:
- **Junior** (local model): Tries to solve independently, learns from failures
- **Senior** (frontier model): Intervenes only when junior is stuck
- **Progressive disclosure**: Senior provides just enough guidance to unblock, not full solution

**Adaptive Escalation**:
- Start minimal (cheapest path)
- Escalate only as needed (adaptive cost)
- Each checkpoint adds value proportional to struggle (targeted help)

**Cost Awareness**:
- Track when checkpoints help vs. hinder
- Measure if adaptive system beats direct frontier solve
- Optimize checkpoint_interval and max_iterations based on data

---

## Next Steps

1. **Benchmark with minimal specs**: Run subset of HumanEval to measure:
   - One-shot success rate drop (how much did detailed specs inflate it?)
   - Avg iterations to solve (realistic difficulty)
   - Checkpoint effectiveness (do they actually help?)

2. **Cost analysis**: Compare:
   - Problems solved without checkpoints (pure local, $0)
   - Problems needing checkpoints (adaptive escalation, $0.05-0.20)
   - Direct frontier solve (baseline, $0.01)

3. **Adaptive tuning**: Based on data, optimize:
   - checkpoint_interval (currently 5 - too frequent?)
   - max_iterations (currently 50 - too high?)
   - Progressive spec verbosity (currently moderate)

4. **Senior/junior optimization**: Consider:
   - Should local model ever query frontier directly?
   - Can checkpoints build long-term local capability?
   - Is there a better escalation pattern?
