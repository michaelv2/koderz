```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                        â•‘
â•‘   â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                    â•‘
â•‘   â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ–ˆâ•”â•                    â•‘
â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ–ˆâ•”â•                     â•‘
â•‘   â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•”â•                      â•‘
â•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                    â•‘
â•‘   â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•                    â•‘
â•‘                                                                        â•‘
â•‘              Multi-Model Swarm Experiment Framework                    â•‘
â•‘                                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

Koderz is a **multi-model swarm experiment framework** that orchestrates coding experiments using local models (CodeLlama 70B, gpt-oss:20b) or small frontier models (GPT-4o-mini, Claude Haiku) for iterations, supervised by frontier models (Claude Sonnet/Opus) for checkpoints, with all experimental data tracked via the `claude-cortex-core` MCP server.

**What's New:**
- ðŸŽ¯ **gpt-oss:20b default spec model**: Validated 100% first-try success rate, 37% faster than Sonnet, zero cost
- âœ… **Three-tier model system**: Local (free), Small Frontier (cheap), Full Frontier (expensive)
- âœ… **Spec reuse feature**: Save 60-75% on costs by reusing specifications across experiments
- âœ… **Code extraction utilities**: Automatically extract Python code from markdown/text wrappers
- âœ… **Debug mode**: Save all iteration outputs, extracted code, and test results for analysis
- âœ… **Flexible model selection**: Use any model for any phase (spec, iterations, checkpoints)

## Core Research Question

Can we achieve comparable results to expensive frontier models by giving cheaper local or small frontier models unlimited time and iterative refinement?

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  koderz (Python CLI)             â”‚
â”‚  - Experiment orchestration      â”‚
â”‚  - Model clients:                â”‚
â”‚    â€¢ Ollama (local)              â”‚
â”‚    â€¢ Anthropic API (frontier)    â”‚
â”‚    â€¢ OpenAI API (small frontier) â”‚
â”‚  - HumanEval benchmark harness   â”‚
â”‚  - MCP client â†’ cortex           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ (MCP protocol)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  claude-cortex-core              â”‚
â”‚  - Memory storage/retrieval      â”‚
â”‚  - 15 existing tools (no changes)â”‚
â”‚  - SQLite backend                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Tiers

1. **Local** (Free) - Ollama models (gpt-oss:20b, CodeLlama 70B, Llama 3.3, qwen2.5-coder:32b)
2. **Small Frontier** (Cheap) - GPT-4o-mini ($0.15/$0.60 per 1M tokens), Claude Haiku ($0.80/$4.00 per 1M tokens)
3. **Full Frontier** (Expensive) - Claude Opus ($15/$75 per 1M tokens), Claude Sonnet ($3/$15 per 1M tokens), GPT-4o ($2.50/$10 per 1M tokens)

### Recommended Model Configuration ðŸŽ¯

**Default (Validated for Best Results):**
```bash
--frontier-spec-model "gpt-oss:20b"              # Spec generation (FREE)
--local-model "qwen2.5-coder:32b"                # Implementation (FREE)
--frontier-checkpoint-model "claude-sonnet-4-5"  # Checkpoints (paid)
```

**Why gpt-oss:20b for specs?**
- âœ… **100% first-try success** validated on 20 HumanEval problems
- âœ… **37% faster** than Claude Sonnet 4.5 (16.4s vs 26.5s avg)
- âœ… **30% more detailed** specs (6,625 chars vs 5,077 chars)
- âœ… **Zero cost** vs $0.024 per spec with Sonnet
- âœ… **Production-quality formatting**: markdown tables, code examples, comprehensive edge cases

See [SPEC_VALIDATION_GPTOSS.md](SPEC_VALIDATION_GPTOSS.md) for full validation results and analysis.

**Alternative Configurations:**

*For maximum quality (paid):*
```bash
--frontier-spec-model "claude-opus-4-5"          # Most comprehensive specs
--local-model "gpt-4o-mini"                      # Fast, cheap iterations
--frontier-checkpoint-model "claude-sonnet-4-5"  # Strong guidance
```

*For maximum speed (free):*
```bash
--frontier-spec-model "gpt-oss:20b"              # Fast, detailed specs
--local-model "codellama:70b"                    # Fast local model
--frontier-checkpoint-model "claude-haiku-4-5"   # Cheap checkpoints
```

## Workflow

### Phase 1: Spec Generation
1. Load problem from HumanEval benchmark
2. Spec model (default: gpt-oss:20b) generates detailed implementation spec
3. Store spec in cortex via `remember` tool

### Phase 2: Iterative Execution (Local Model Swarm)
1. Local model (via Ollama) generates solution based on spec
2. Execute solution against tests
3. Store iteration in cortex
4. If tests pass â†’ Complete (Phase 4)
5. If tests fail â†’ Feed errors back to local model, repeat

### Phase 3: Frontier Checkpoint (Every 5 Iterations)
1. Query cortex for last 5 iterations
2. Frontier model reviews attempts and provides guidance
3. Store checkpoint in cortex
4. Feed guidance back to local model

### Phase 4: Completion & Analysis
1. Calculate cost analysis (frontier vs local)
2. Store final result in cortex
3. Display savings compared to frontier-only approach

## Installation

### Prerequisites

1. **Node.js 18+** (for claude-cortex-core)
2. **Python 3.10+**
3. **Ollama** - Local model server
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull codellama:70b
   ```
4. **HumanEval dataset**
   ```bash
   # Download from https://github.com/openai/human-eval
   # Place HumanEval.jsonl in koderz/data/
   ```

### Install Koderz

```bash
cd koderz

# Using pip
pip install -e .

# Or using Poetry
poetry install
```

### Configure Environment

You can use environment variables directly or create a `.env` file:

**Option 1: Bash environment variables (recommended)**

```bash
export ANTHROPIC_API_KEY=sk-ant-...
export OPENAI_API_KEY=sk-proj-...  # Optional, for small frontier models
export OLLAMA_HOST=http://localhost:11434
export CORTEX_PATH=/path/to/claude-cortex-core/dist/index.js
```

**Option 2: .env file**

```bash
cp .env.example .env
# Edit .env with your keys
```

### Build Cortex Core

```bash
cd ../claude-cortex-core
npm install
npm run build
```

## Usage

### Run Single Experiment

**Default (gpt-oss:20b spec, local iterations - RECOMMENDED):**
```bash
poetry run koderz run --problem-id "HumanEval/0"
# Uses: gpt-oss:20b for spec (free), codellama:70b for iterations (free)
```

**All free local models:**
```bash
poetry run koderz run --problem-id "HumanEval/0" \
  --frontier-spec-model "gpt-oss:20b" \
  --local-model "qwen2.5-coder:32b" \
  --frontier-checkpoint-model "gpt-oss:20b"
```

**Maximum quality (paid):**
```bash
poetry run koderz run --problem-id "HumanEval/0" \
  --frontier-spec-model "claude-opus-4-5" \
  --local-model "gpt-4o-mini" \
  --frontier-checkpoint-model "claude-sonnet-4-5"
```

**Reuse existing spec (save 60-75% on costs):**
```bash
poetry run koderz run --problem-id "HumanEval/0" \
  --local-model "gpt-4o-mini" \
  --reuse-spec
```

**Debug mode (save all outputs for analysis):**
```bash
poetry run koderz run --problem-id "HumanEval/0" \
  --debug \
  --debug-dir ./debug_output
```

### Options

- `--local-model` - Model for iterations (default: `codellama:70b`)
  - Local: `gpt-oss:20b` (recommended for speed), `qwen2.5-coder:32b`, `codellama:70b`, `llama3.3:70b`
  - Small frontier: `gpt-4o-mini`, `claude-haiku-4-5`
  - Full frontier: `claude-sonnet-4-5`, `gpt-4o`
- `--frontier-spec-model` - Model for spec generation (default: `gpt-oss:20b`)
  - Recommended: `gpt-oss:20b` (validated 100% first-try success, free)
  - Alternative: `claude-sonnet-4-5`, `claude-opus-4-5`, `qwen2.5-coder:32b`
- `--frontier-checkpoint-model` - Model for checkpoints (default: `claude-sonnet-4-5`)
- `--max-iterations` - Max iterations (default: 50)
- `--checkpoint-interval` - Checkpoint every N iterations (default: 5)
- `--reuse-spec` - Reuse existing spec from Cortex instead of regenerating (recommended for benchmarks)
- `--debug` - Enable debug mode: saves raw outputs, extracted code, and test results
- `--debug-dir` - Directory for debug outputs (default: `./debug`)

### Run Benchmark

```bash
poetry run koderz benchmark --start 0 --end 10 \
  --local-model "gpt-4o-mini"
```

Runs experiments on HumanEval problems 0-9.

### List Problems

```bash
poetry run koderz list-problems
```

### Analyze Experiment

```bash
poetry run koderz analyze exp_abc12345
```

Query cortex for experiment data. For full details, use Claude Code:

```bash
claude
> /recall query:exp_abc12345
```

## Example Output

**Example 1: Default approach (spec=gpt-oss:20b, iterations=CodeLlama, checkpoint=Sonnet)**

```
============================================================
Starting Experiment: exp_a1b2c3d4
Problem: HumanEval/0
============================================================

Phase 1: Generating spec with gpt-oss:20b...
  Spec generated (cost: $0.00)
  Stored in cortex

Phase 2: Iterative execution with codellama:70b...
  Iteration 1/50...
    [INFO] Code extracted from markdown/text wrapper
    âœ— Failed: IndexError: list index out of range
  Iteration 2/50...
    âœ— Failed: Expected True, got False
  Iteration 3/50...
    [INFO] Code extracted from markdown/text wrapper
    âœ— Failed: AssertionError
  Iteration 4/50...
    âœ— Failed: Expected True, got False
  Iteration 5/50...
    âœ— Failed: IndexError

  Checkpoint 1...
    Guidance received from claude-sonnet-4-5

  Iteration 6/50...
    [INFO] Code extracted from markdown/text wrapper
    âœ— Failed: Expected True, got False
  Iteration 7/50...
    âœ— Failed: AssertionError
  Iteration 8/50...
    âœ“ SUCCESS! All tests passed.

============================================================
Experiment Complete: exp_a1b2c3d4
============================================================
Success: True
Iterations: 8

Cost Analysis:
  Actual Total: $0.0023
    - Full Frontier: $0.0023 (checkpoint)
    - Small Frontier: $0.0000 (0 calls)
    - Local: $0.0000 (spec + 8 iterations - free)

  Frontier-Only Estimate: $0.1368
  Savings: $0.1345 (98.3%)
```

**Example 2: Spec reuse (second run on same problem)**

```
Phase 1: Looking for existing spec for HumanEval/0...
  Found existing spec (generated by claude-sonnet-4-5)
  Reusing spec (cost: $0.00 - saved!)

Phase 2: Iterative execution with gpt-4o-mini...
  [...]
```

**Example 3: Debug mode output**

```
Phase 2: Iterative execution with codellama:70b...
  Iteration 1/50...
    [DEBUG] Raw output saved to debug/exp_a1b2c3d4_iter001_raw.txt
    [INFO] Code extracted from markdown/text wrapper
    [DEBUG] Extracted code saved to debug/exp_a1b2c3d4_iter001_code.py
    [DEBUG] Code preview: def has_close_elements(numbers: List[float], threshold...
    [DEBUG] Test result saved to debug/exp_a1b2c3d4_iter001_result.txt
    âœ— Failed: IndexError: list index out of range
```

## Project Structure

```
koderz/
â”œâ”€â”€ pyproject.toml          # Poetry dependencies
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ koderz/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py              # CLI entry point
â”‚   â”œâ”€â”€ orchestrator.py     # Experiment orchestration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ local.py        # Ollama client
â”‚   â”‚   â”œâ”€â”€ frontier.py     # Anthropic API client
â”‚   â”‚   â”œâ”€â”€ openai_client.py # OpenAI API client (GPT-4o, GPT-4o-mini)
â”‚   â”‚   â”œâ”€â”€ registry.py     # Model metadata and tier definitions
â”‚   â”‚   â””â”€â”€ factory.py      # Client factory pattern
â”‚   â”œâ”€â”€ cortex/
â”‚   â”‚   â””â”€â”€ client.py       # MCP client for cortex-core
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â””â”€â”€ humaneval.py    # HumanEval loader & executor
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â””â”€â”€ cost.py         # Cost analysis with tier tracking
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ code_extraction.py # Code extraction from markdown/text
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ HumanEval.jsonl # Sample problems
â””â”€â”€ tests/
    â””â”€â”€ test_orchestrator.py
```

## Testing & Verification

### Quick Verification

```bash
# Test all components
poetry run python -c "
from koderz.models.factory import ModelFactory
from koderz.cortex.client import CortexClient
import os
print('âœ“ All imports successful')
"

# List available problems
poetry run koderz list-problems

# Test code execution
poetry run python -c "
from koderz.benchmarks.humaneval import execute_solution
result = execute_solution('def f(): return 42', 'assert f() == 42')
print(f'âœ“ Code execution: {result[\"success\"]}')
"
```

### Individual Component Tests

**Test OpenAI client:**
```bash
poetry run python -c "
from koderz.models.openai_client import OpenAIClient
import os
client = OpenAIClient(os.getenv('OPENAI_API_KEY'))
result = client.generate_spec('Write a function that adds two numbers', model='gpt-4o-mini')
print(f'Spec: {result[\"spec\"][:100]}...')
print(f'Cost: \${result[\"cost\"]:.6f}')
"
```

**Test model factory:**
```bash
poetry run python -c "
from koderz.models.factory import ModelFactory
import os
factory = ModelFactory(
    anthropic_api_key=os.getenv('ANTHROPIC_API_KEY'),
    openai_api_key=os.getenv('OPENAI_API_KEY')
)
print(f'âœ“ Anthropic client: {type(factory.get_client(\"claude-opus-4-5\"))}')
print(f'âœ“ OpenAI client: {type(factory.get_client(\"gpt-4o-mini\"))}')
print(f'âœ“ Ollama client: {type(factory.get_client(\"codellama:70b\"))}')
"
```

**Test MCP connection:**
```bash
poetry run python -c "
from koderz.cortex.client import CortexClient
import asyncio, os

async def test():
    cortex = CortexClient(os.getenv('CORTEX_PATH'))
    result = await cortex.remember(
        title='Test', content='Testing', category='note'
    )
    print(f'âœ“ Memory created: {result}')

asyncio.run(test())
"
```

## Key Features

### âœ… Three-Tier Model System
- **Local models** (free) - CodeLlama, Llama 3.3 via Ollama
- **Small frontier models** (cheap) - GPT-4o-mini, Claude Haiku
- **Full frontier models** (expensive) - Claude Opus/Sonnet, GPT-4o
- Mix and match models for different phases (spec, iterations, checkpoints)

### âœ… Spec Reuse
- Save specifications in Cortex for reuse across experiments
- 60-75% cost savings on multi-model comparisons
- Zero-cost spec retrieval from memory
- Consistent baseline across experiments

### âœ… Memory Persistence
All experiment data stored in Cortex:
- Specifications with cost metadata
- Each iteration attempt with test results
- Checkpoint reviews and guidance
- Final results and cost analysis

Query via Claude Code:
```bash
claude
> Examine failures from experiment exp_a1b2c3d4
```

### âœ… Cost Tracking by Tier
- Separate tracking for local, small frontier, and full frontier costs
- Detailed breakdown in experiment results
- Savings calculation vs frontier-only baseline
- Model usage statistics

### âœ… Code Extraction
- Automatically extracts Python code from markdown fenced blocks (` ```python `)
- Handles generic code blocks and plain text responses
- Validates syntax before execution using AST parsing
- Fallback strategies for various output formats

### âœ… Debug Mode
- Save all raw model outputs (`*_raw.txt`)
- Save extracted Python code (`*_code.py`)
- Save test results (`*_result.txt`)
- Save checkpoint guidance files
- Helpful INFO/DEBUG/WARNING messages during execution

### âœ… Flexible Workflow
- Configurable checkpoint interval
- Max iteration limits
- Custom model selection per phase
- Async architecture ready for parallelization

## Documentation

- **[SPEC_VALIDATION_GPTOSS.md](SPEC_VALIDATION_GPTOSS.md)** - Validation results: 100% first-try success with gpt-oss:20b specs
- **[SPEC_REUSE_FEATURE.md](SPEC_REUSE_FEATURE.md)** - Comprehensive guide to spec reuse feature with examples and cost savings
- **[SPEC_QWEN_VS_GPTOSS_ANALYSIS.md](SPEC_QWEN_VS_GPTOSS_ANALYSIS.md)** - Comparison analysis: gpt-oss:20b vs qwen2.5-coder:32b for specs
- **[SPEC_3WAY_ANALYSIS.md](SPEC_3WAY_ANALYSIS.md)** - Three-way comparison: Sonnet vs llama3.3:70b vs qwen2.5-coder:32b
- **[QUICKSTART.md](QUICKSTART.md)** - 5-minute setup guide
- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Technical deep dive
- **[TODO.md](TODO.md)** - Future roadmap

## Future Enhancements

### Phase 2 (Post-MVP)
- Multi-agent swarm (generator, critic, tester roles)
- Beam search (try N solutions in parallel)
- Quality-based checkpointing
- Support for MBPP, SWE-bench benchmarks

### Phase 3 (Research)
- Tree search with MCTS
- Meta-learning from past experiments
- Cost prediction models
- Comparative analysis dashboards

## Cost Analysis

The framework tracks costs across three tiers:
- **Full Frontier**: Claude Opus/Sonnet, GPT-4o ($2.50-$75 per 1M tokens)
- **Small Frontier**: GPT-4o-mini, Claude Haiku ($0.15-$4.00 per 1M tokens)
- **Local**: $0 (electricity only, not tracked)
- **Estimated frontier-only cost**: What it would cost if full frontier did all work
- **Savings**: Difference between actual and frontier-only baseline

### Cost Projections

**Per Experiment (various configurations):**
- Full frontier only: $0.10-0.15 (baseline)
- **Default (gpt-oss:20b spec + local iterations): $0.00-0.005** (96-100% savings) âœ¨
- Hybrid (Sonnet checkpoints): $0.002-0.01 (90-98% savings)
- Hybrid (Opus spec + small frontier iterations): $0.04-0.06 (60-70% savings)
- All small frontier: $0.01-0.02 (85-90% savings)

**Benchmark (164 problems):**
- Full frontier only: $16-25
- **Default (gpt-oss:20b + local): $0.00** (100% savings on specs) âœ¨
- Hybrid (gpt-oss:20b spec + Sonnet checkpoints): $0.50-1.50 (90-95% savings)
- Traditional hybrid (Sonnet spec + local): $3-5 (75-85% savings)

**Spec Generation Comparison (164 problems):**
- Claude Sonnet 4.5: $3.94 (72 minutes)
- **gpt-oss:20b: $0.00 (45 minutes)** - **37% faster, zero cost** âœ¨
- Savings: $3.94 + 27 minutes per benchmark

See [SPEC_REUSE_FEATURE.md](SPEC_REUSE_FEATURE.md) for detailed examples.

## License

MIT

## Contributing

Contributions welcome! This is an experimental research framework.

## Citation

If you use Koderz in research, please cite:

```bibtex
@software{koderz2025,
  title={Koderz: Multi-Model Swarm Experiment Framework},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/koderz}
}
```
